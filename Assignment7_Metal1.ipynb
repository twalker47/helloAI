{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twalker47/helloAI/blob/main/Assignment7_Metal1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CzeLeqEaNj8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417152a8-5375-4347-9f81-4880316b30b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16225777.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 273546.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5009480.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 2586830.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training using CPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.315087  [   64/60000]\n",
            "loss: 2.295753  [ 6464/60000]\n",
            "loss: 2.283997  [12864/60000]\n",
            "loss: 2.273624  [19264/60000]\n",
            "loss: 2.247981  [25664/60000]\n",
            "loss: 2.223841  [32064/60000]\n",
            "loss: 2.236733  [38464/60000]\n",
            "loss: 2.198424  [44864/60000]\n",
            "loss: 2.199429  [51264/60000]\n",
            "loss: 2.165088  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.169720  [   64/60000]\n",
            "loss: 2.153992  [ 6464/60000]\n",
            "loss: 2.103750  [12864/60000]\n",
            "loss: 2.124129  [19264/60000]\n",
            "loss: 2.062160  [25664/60000]\n",
            "loss: 2.005017  [32064/60000]\n",
            "loss: 2.036175  [38464/60000]\n",
            "loss: 1.949463  [44864/60000]\n",
            "loss: 1.959880  [51264/60000]\n",
            "loss: 1.888249  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.916090  [   64/60000]\n",
            "loss: 1.880444  [ 6464/60000]\n",
            "loss: 1.770064  [12864/60000]\n",
            "loss: 1.817596  [19264/60000]\n",
            "loss: 1.702260  [25664/60000]\n",
            "loss: 1.655524  [32064/60000]\n",
            "loss: 1.679223  [38464/60000]\n",
            "loss: 1.576704  [44864/60000]\n",
            "loss: 1.605936  [51264/60000]\n",
            "loss: 1.508223  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.588124  [   64/60000]\n",
            "loss: 1.550781  [ 6464/60000]\n",
            "loss: 1.406757  [12864/60000]\n",
            "loss: 1.483023  [19264/60000]\n",
            "loss: 1.361207  [25664/60000]\n",
            "loss: 1.359708  [32064/60000]\n",
            "loss: 1.374217  [38464/60000]\n",
            "loss: 1.296021  [44864/60000]\n",
            "loss: 1.332628  [51264/60000]\n",
            "loss: 1.246213  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.337587  [   64/60000]\n",
            "loss: 1.316772  [ 6464/60000]\n",
            "loss: 1.155553  [12864/60000]\n",
            "loss: 1.265206  [19264/60000]\n",
            "loss: 1.132497  [25664/60000]\n",
            "loss: 1.162276  [32064/60000]\n",
            "loss: 1.183737  [38464/60000]\n",
            "loss: 1.116771  [44864/60000]\n",
            "loss: 1.159401  [51264/60000]\n",
            "loss: 1.087823  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.168173  [   64/60000]\n",
            "loss: 1.166569  [ 6464/60000]\n",
            "loss: 0.988840  [12864/60000]\n",
            "loss: 1.126431  [19264/60000]\n",
            "loss: 0.990529  [25664/60000]\n",
            "loss: 1.028433  [32064/60000]\n",
            "loss: 1.064698  [38464/60000]\n",
            "loss: 1.000706  [44864/60000]\n",
            "loss: 1.045157  [51264/60000]\n",
            "loss: 0.986255  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.049478  [   64/60000]\n",
            "loss: 1.068991  [ 6464/60000]\n",
            "loss: 0.874283  [12864/60000]\n",
            "loss: 1.032972  [19264/60000]\n",
            "loss: 0.901417  [25664/60000]\n",
            "loss: 0.934470  [32064/60000]\n",
            "loss: 0.987117  [38464/60000]\n",
            "loss: 0.924673  [44864/60000]\n",
            "loss: 0.965499  [51264/60000]\n",
            "loss: 0.917637  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.962316  [   64/60000]\n",
            "loss: 1.001825  [ 6464/60000]\n",
            "loss: 0.792551  [12864/60000]\n",
            "loss: 0.966385  [19264/60000]\n",
            "loss: 0.843224  [25664/60000]\n",
            "loss: 0.866059  [32064/60000]\n",
            "loss: 0.933085  [38464/60000]\n",
            "loss: 0.873576  [44864/60000]\n",
            "loss: 0.907861  [51264/60000]\n",
            "loss: 0.868081  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.896057  [   64/60000]\n",
            "loss: 0.952132  [ 6464/60000]\n",
            "loss: 0.731605  [12864/60000]\n",
            "loss: 0.916937  [19264/60000]\n",
            "loss: 0.802826  [25664/60000]\n",
            "loss: 0.814555  [32064/60000]\n",
            "loss: 0.892642  [38464/60000]\n",
            "loss: 0.837829  [44864/60000]\n",
            "loss: 0.864825  [51264/60000]\n",
            "loss: 0.830268  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.843383  [   64/60000]\n",
            "loss: 0.912772  [ 6464/60000]\n",
            "loss: 0.684196  [12864/60000]\n",
            "loss: 0.878965  [19264/60000]\n",
            "loss: 0.772538  [25664/60000]\n",
            "loss: 0.774818  [32064/60000]\n",
            "loss: 0.860174  [38464/60000]\n",
            "loss: 0.811624  [44864/60000]\n",
            "loss: 0.831682  [51264/60000]\n",
            "loss: 0.800158  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.800068  [   64/60000]\n",
            "loss: 0.879692  [ 6464/60000]\n",
            "loss: 0.645977  [12864/60000]\n",
            "loss: 0.848778  [19264/60000]\n",
            "loss: 0.748551  [25664/60000]\n",
            "loss: 0.743272  [32064/60000]\n",
            "loss: 0.832644  [38464/60000]\n",
            "loss: 0.791281  [44864/60000]\n",
            "loss: 0.805181  [51264/60000]\n",
            "loss: 0.775187  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.763138  [   64/60000]\n",
            "loss: 0.850807  [ 6464/60000]\n",
            "loss: 0.614165  [12864/60000]\n",
            "loss: 0.824108  [19264/60000]\n",
            "loss: 0.728660  [25664/60000]\n",
            "loss: 0.717521  [32064/60000]\n",
            "loss: 0.808275  [38464/60000]\n",
            "loss: 0.774367  [44864/60000]\n",
            "loss: 0.783209  [51264/60000]\n",
            "loss: 0.753673  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.730877  [   64/60000]\n",
            "loss: 0.824905  [ 6464/60000]\n",
            "loss: 0.587038  [12864/60000]\n",
            "loss: 0.803380  [19264/60000]\n",
            "loss: 0.711446  [25664/60000]\n",
            "loss: 0.695968  [32064/60000]\n",
            "loss: 0.785920  [38464/60000]\n",
            "loss: 0.759640  [44864/60000]\n",
            "loss: 0.764404  [51264/60000]\n",
            "loss: 0.734660  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.702189  [   64/60000]\n",
            "loss: 0.801284  [ 6464/60000]\n",
            "loss: 0.563433  [12864/60000]\n",
            "loss: 0.785544  [19264/60000]\n",
            "loss: 0.696242  [25664/60000]\n",
            "loss: 0.677548  [32064/60000]\n",
            "loss: 0.765074  [38464/60000]\n",
            "loss: 0.746322  [44864/60000]\n",
            "loss: 0.747918  [51264/60000]\n",
            "loss: 0.717485  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.676387  [   64/60000]\n",
            "loss: 0.779628  [ 6464/60000]\n",
            "loss: 0.542540  [12864/60000]\n",
            "loss: 0.769767  [19264/60000]\n",
            "loss: 0.682601  [25664/60000]\n",
            "loss: 0.661629  [32064/60000]\n",
            "loss: 0.745392  [38464/60000]\n",
            "loss: 0.734116  [44864/60000]\n",
            "loss: 0.733294  [51264/60000]\n",
            "loss: 0.701800  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.653092  [   64/60000]\n",
            "loss: 0.759608  [ 6464/60000]\n",
            "loss: 0.523846  [12864/60000]\n",
            "loss: 0.755458  [19264/60000]\n",
            "loss: 0.670367  [25664/60000]\n",
            "loss: 0.647779  [32064/60000]\n",
            "loss: 0.726672  [38464/60000]\n",
            "loss: 0.722859  [44864/60000]\n",
            "loss: 0.720266  [51264/60000]\n",
            "loss: 0.687218  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.632012  [   64/60000]\n",
            "loss: 0.741144  [ 6464/60000]\n",
            "loss: 0.506987  [12864/60000]\n",
            "loss: 0.742469  [19264/60000]\n",
            "loss: 0.659215  [25664/60000]\n",
            "loss: 0.635646  [32064/60000]\n",
            "loss: 0.708837  [38464/60000]\n",
            "loss: 0.712667  [44864/60000]\n",
            "loss: 0.708619  [51264/60000]\n",
            "loss: 0.673641  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.612869  [   64/60000]\n",
            "loss: 0.723949  [ 6464/60000]\n",
            "loss: 0.491719  [12864/60000]\n",
            "loss: 0.730557  [19264/60000]\n",
            "loss: 0.649150  [25664/60000]\n",
            "loss: 0.624979  [32064/60000]\n",
            "loss: 0.691926  [38464/60000]\n",
            "loss: 0.703601  [44864/60000]\n",
            "loss: 0.698339  [51264/60000]\n",
            "loss: 0.660936  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.595417  [   64/60000]\n",
            "loss: 0.707933  [ 6464/60000]\n",
            "loss: 0.477740  [12864/60000]\n",
            "loss: 0.719640  [19264/60000]\n",
            "loss: 0.640066  [25664/60000]\n",
            "loss: 0.615495  [32064/60000]\n",
            "loss: 0.675928  [38464/60000]\n",
            "loss: 0.695626  [44864/60000]\n",
            "loss: 0.689194  [51264/60000]\n",
            "loss: 0.648985  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.579704  [   64/60000]\n",
            "loss: 0.693078  [ 6464/60000]\n",
            "loss: 0.465077  [12864/60000]\n",
            "loss: 0.709494  [19264/60000]\n",
            "loss: 0.631936  [25664/60000]\n",
            "loss: 0.607130  [32064/60000]\n",
            "loss: 0.660905  [38464/60000]\n",
            "loss: 0.688699  [44864/60000]\n",
            "loss: 0.681134  [51264/60000]\n",
            "loss: 0.637689  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.565298  [   64/60000]\n",
            "loss: 0.679313  [ 6464/60000]\n",
            "loss: 0.453584  [12864/60000]\n",
            "loss: 0.700242  [19264/60000]\n",
            "loss: 0.624355  [25664/60000]\n",
            "loss: 0.599651  [32064/60000]\n",
            "loss: 0.646814  [38464/60000]\n",
            "loss: 0.682790  [44864/60000]\n",
            "loss: 0.674160  [51264/60000]\n",
            "loss: 0.626951  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.552161  [   64/60000]\n",
            "loss: 0.666563  [ 6464/60000]\n",
            "loss: 0.443064  [12864/60000]\n",
            "loss: 0.691651  [19264/60000]\n",
            "loss: 0.617302  [25664/60000]\n",
            "loss: 0.592864  [32064/60000]\n",
            "loss: 0.633656  [38464/60000]\n",
            "loss: 0.677768  [44864/60000]\n",
            "loss: 0.668163  [51264/60000]\n",
            "loss: 0.616755  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.540039  [   64/60000]\n",
            "loss: 0.654720  [ 6464/60000]\n",
            "loss: 0.433393  [12864/60000]\n",
            "loss: 0.683752  [19264/60000]\n",
            "loss: 0.610703  [25664/60000]\n",
            "loss: 0.586670  [32064/60000]\n",
            "loss: 0.621393  [38464/60000]\n",
            "loss: 0.673712  [44864/60000]\n",
            "loss: 0.663120  [51264/60000]\n",
            "loss: 0.606977  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.528822  [   64/60000]\n",
            "loss: 0.643729  [ 6464/60000]\n",
            "loss: 0.424469  [12864/60000]\n",
            "loss: 0.676424  [19264/60000]\n",
            "loss: 0.604505  [25664/60000]\n",
            "loss: 0.580869  [32064/60000]\n",
            "loss: 0.609996  [38464/60000]\n",
            "loss: 0.670458  [44864/60000]\n",
            "loss: 0.658905  [51264/60000]\n",
            "loss: 0.597569  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.518300  [   64/60000]\n",
            "loss: 0.633497  [ 6464/60000]\n",
            "loss: 0.416194  [12864/60000]\n",
            "loss: 0.669450  [19264/60000]\n",
            "loss: 0.598454  [25664/60000]\n",
            "loss: 0.575394  [32064/60000]\n",
            "loss: 0.599365  [38464/60000]\n",
            "loss: 0.667877  [44864/60000]\n",
            "loss: 0.655260  [51264/60000]\n",
            "loss: 0.588466  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.508420  [   64/60000]\n",
            "loss: 0.623980  [ 6464/60000]\n",
            "loss: 0.408479  [12864/60000]\n",
            "loss: 0.662848  [19264/60000]\n",
            "loss: 0.592560  [25664/60000]\n",
            "loss: 0.570121  [32064/60000]\n",
            "loss: 0.589409  [38464/60000]\n",
            "loss: 0.665924  [44864/60000]\n",
            "loss: 0.652191  [51264/60000]\n",
            "loss: 0.579614  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.499073  [   64/60000]\n",
            "loss: 0.615133  [ 6464/60000]\n",
            "loss: 0.401243  [12864/60000]\n",
            "loss: 0.656614  [19264/60000]\n",
            "loss: 0.586708  [25664/60000]\n",
            "loss: 0.565123  [32064/60000]\n",
            "loss: 0.580187  [38464/60000]\n",
            "loss: 0.664481  [44864/60000]\n",
            "loss: 0.649588  [51264/60000]\n",
            "loss: 0.570981  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.490197  [   64/60000]\n",
            "loss: 0.606831  [ 6464/60000]\n",
            "loss: 0.394458  [12864/60000]\n",
            "loss: 0.650667  [19264/60000]\n",
            "loss: 0.580910  [25664/60000]\n",
            "loss: 0.560231  [32064/60000]\n",
            "loss: 0.571662  [38464/60000]\n",
            "loss: 0.663478  [44864/60000]\n",
            "loss: 0.647384  [51264/60000]\n",
            "loss: 0.562585  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.481721  [   64/60000]\n",
            "loss: 0.599076  [ 6464/60000]\n",
            "loss: 0.388098  [12864/60000]\n",
            "loss: 0.644893  [19264/60000]\n",
            "loss: 0.575197  [25664/60000]\n",
            "loss: 0.555391  [32064/60000]\n",
            "loss: 0.563774  [38464/60000]\n",
            "loss: 0.662880  [44864/60000]\n",
            "loss: 0.645503  [51264/60000]\n",
            "loss: 0.554474  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.473653  [   64/60000]\n",
            "loss: 0.591787  [ 6464/60000]\n",
            "loss: 0.382145  [12864/60000]\n",
            "loss: 0.639373  [19264/60000]\n",
            "loss: 0.569536  [25664/60000]\n",
            "loss: 0.550618  [32064/60000]\n",
            "loss: 0.556450  [38464/60000]\n",
            "loss: 0.662579  [44864/60000]\n",
            "loss: 0.643893  [51264/60000]\n",
            "loss: 0.546635  [57664/60000]\n",
            "completed training in ... 364.4668323993683s\n",
            "starting timer for testing using CPU...\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.551098 \n",
            "\n",
            "completed testing in ... 1.4947834014892578s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = NeuralNetwork().to(cpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using CPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, cpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using CPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, cpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = NeuralNetwork().to(gpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using GPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, gpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using GPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, gpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "id": "ZzDl_gD-o-DU",
        "outputId": "9b247242-1493-44e5-d411-9e6475192370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training using GPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.312085  [   64/60000]\n",
            "loss: 2.303165  [ 6464/60000]\n",
            "loss: 2.276104  [12864/60000]\n",
            "loss: 2.263016  [19264/60000]\n",
            "loss: 2.264467  [25664/60000]\n",
            "loss: 2.224370  [32064/60000]\n",
            "loss: 2.233478  [38464/60000]\n",
            "loss: 2.206097  [44864/60000]\n",
            "loss: 2.200204  [51264/60000]\n",
            "loss: 2.157695  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.175100  [   64/60000]\n",
            "loss: 2.168899  [ 6464/60000]\n",
            "loss: 2.103078  [12864/60000]\n",
            "loss: 2.114273  [19264/60000]\n",
            "loss: 2.074354  [25664/60000]\n",
            "loss: 2.012125  [32064/60000]\n",
            "loss: 2.037055  [38464/60000]\n",
            "loss: 1.966810  [44864/60000]\n",
            "loss: 1.972680  [51264/60000]\n",
            "loss: 1.887959  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.927730  [   64/60000]\n",
            "loss: 1.903908  [ 6464/60000]\n",
            "loss: 1.773594  [12864/60000]\n",
            "loss: 1.810648  [19264/60000]\n",
            "loss: 1.707127  [25664/60000]\n",
            "loss: 1.656930  [32064/60000]\n",
            "loss: 1.673974  [38464/60000]\n",
            "loss: 1.579952  [44864/60000]\n",
            "loss: 1.604446  [51264/60000]\n",
            "loss: 1.488971  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.583594  [   64/60000]\n",
            "loss: 1.553647  [ 6464/60000]\n",
            "loss: 1.388824  [12864/60000]\n",
            "loss: 1.457907  [19264/60000]\n",
            "loss: 1.338363  [25664/60000]\n",
            "loss: 1.344546  [32064/60000]\n",
            "loss: 1.351889  [38464/60000]\n",
            "loss: 1.283093  [44864/60000]\n",
            "loss: 1.317298  [51264/60000]\n",
            "loss: 1.213765  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.328603  [   64/60000]\n",
            "loss: 1.314117  [ 6464/60000]\n",
            "loss: 1.135885  [12864/60000]\n",
            "loss: 1.238333  [19264/60000]\n",
            "loss: 1.109638  [25664/60000]\n",
            "loss: 1.152045  [32064/60000]\n",
            "loss: 1.166293  [38464/60000]\n",
            "loss: 1.110359  [44864/60000]\n",
            "loss: 1.147238  [51264/60000]\n",
            "loss: 1.063072  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.164790  [   64/60000]\n",
            "loss: 1.169906  [ 6464/60000]\n",
            "loss: 0.976200  [12864/60000]\n",
            "loss: 1.107748  [19264/60000]\n",
            "loss: 0.974804  [25664/60000]\n",
            "loss: 1.026357  [32064/60000]\n",
            "loss: 1.054565  [38464/60000]\n",
            "loss: 1.002724  [44864/60000]\n",
            "loss: 1.038539  [51264/60000]\n",
            "loss: 0.970375  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.051971  [   64/60000]\n",
            "loss: 1.076355  [ 6464/60000]\n",
            "loss: 0.867182  [12864/60000]\n",
            "loss: 1.021343  [19264/60000]\n",
            "loss: 0.890199  [25664/60000]\n",
            "loss: 0.936489  [32064/60000]\n",
            "loss: 0.980435  [38464/60000]\n",
            "loss: 0.931523  [44864/60000]\n",
            "loss: 0.962197  [51264/60000]\n",
            "loss: 0.906911  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.967749  [   64/60000]\n",
            "loss: 1.009555  [ 6464/60000]\n",
            "loss: 0.787383  [12864/60000]\n",
            "loss: 0.959069  [19264/60000]\n",
            "loss: 0.832668  [25664/60000]\n",
            "loss: 0.868563  [32064/60000]\n",
            "loss: 0.926739  [38464/60000]\n",
            "loss: 0.882165  [44864/60000]\n",
            "loss: 0.905658  [51264/60000]\n",
            "loss: 0.859650  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.901343  [   64/60000]\n",
            "loss: 0.958030  [ 6464/60000]\n",
            "loss: 0.726140  [12864/60000]\n",
            "loss: 0.911377  [19264/60000]\n",
            "loss: 0.790781  [25664/60000]\n",
            "loss: 0.815688  [32064/60000]\n",
            "loss: 0.884688  [38464/60000]\n",
            "loss: 0.846613  [44864/60000]\n",
            "loss: 0.862512  [51264/60000]\n",
            "loss: 0.822000  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.847183  [   64/60000]\n",
            "loss: 0.915865  [ 6464/60000]\n",
            "loss: 0.677598  [12864/60000]\n",
            "loss: 0.873591  [19264/60000]\n",
            "loss: 0.758732  [25664/60000]\n",
            "loss: 0.774247  [32064/60000]\n",
            "loss: 0.850104  [38464/60000]\n",
            "loss: 0.820016  [44864/60000]\n",
            "loss: 0.828635  [51264/60000]\n",
            "loss: 0.791204  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.802013  [   64/60000]\n",
            "loss: 0.880113  [ 6464/60000]\n",
            "loss: 0.638218  [12864/60000]\n",
            "loss: 0.843102  [19264/60000]\n",
            "loss: 0.733324  [25664/60000]\n",
            "loss: 0.741516  [32064/60000]\n",
            "loss: 0.820410  [38464/60000]\n",
            "loss: 0.799171  [44864/60000]\n",
            "loss: 0.801234  [51264/60000]\n",
            "loss: 0.765150  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.763353  [   64/60000]\n",
            "loss: 0.848832  [ 6464/60000]\n",
            "loss: 0.605375  [12864/60000]\n",
            "loss: 0.818098  [19264/60000]\n",
            "loss: 0.712204  [25664/60000]\n",
            "loss: 0.715079  [32064/60000]\n",
            "loss: 0.793960  [38464/60000]\n",
            "loss: 0.781955  [44864/60000]\n",
            "loss: 0.778365  [51264/60000]\n",
            "loss: 0.742749  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.729760  [   64/60000]\n",
            "loss: 0.820661  [ 6464/60000]\n",
            "loss: 0.577416  [12864/60000]\n",
            "loss: 0.797083  [19264/60000]\n",
            "loss: 0.694228  [25664/60000]\n",
            "loss: 0.693478  [32064/60000]\n",
            "loss: 0.770008  [38464/60000]\n",
            "loss: 0.767103  [44864/60000]\n",
            "loss: 0.759039  [51264/60000]\n",
            "loss: 0.722883  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.700146  [   64/60000]\n",
            "loss: 0.795043  [ 6464/60000]\n",
            "loss: 0.553216  [12864/60000]\n",
            "loss: 0.778969  [19264/60000]\n",
            "loss: 0.678785  [25664/60000]\n",
            "loss: 0.675464  [32064/60000]\n",
            "loss: 0.748019  [38464/60000]\n",
            "loss: 0.753830  [44864/60000]\n",
            "loss: 0.742362  [51264/60000]\n",
            "loss: 0.704985  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.673819  [   64/60000]\n",
            "loss: 0.771669  [ 6464/60000]\n",
            "loss: 0.532029  [12864/60000]\n",
            "loss: 0.763030  [19264/60000]\n",
            "loss: 0.665431  [25664/60000]\n",
            "loss: 0.660244  [32064/60000]\n",
            "loss: 0.727635  [38464/60000]\n",
            "loss: 0.741947  [44864/60000]\n",
            "loss: 0.727895  [51264/60000]\n",
            "loss: 0.688660  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.650248  [   64/60000]\n",
            "loss: 0.750351  [ 6464/60000]\n",
            "loss: 0.513262  [12864/60000]\n",
            "loss: 0.748656  [19264/60000]\n",
            "loss: 0.653855  [25664/60000]\n",
            "loss: 0.647257  [32064/60000]\n",
            "loss: 0.708741  [38464/60000]\n",
            "loss: 0.731213  [44864/60000]\n",
            "loss: 0.715235  [51264/60000]\n",
            "loss: 0.673748  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.629061  [   64/60000]\n",
            "loss: 0.730810  [ 6464/60000]\n",
            "loss: 0.496493  [12864/60000]\n",
            "loss: 0.735575  [19264/60000]\n",
            "loss: 0.643734  [25664/60000]\n",
            "loss: 0.635998  [32064/60000]\n",
            "loss: 0.691285  [38464/60000]\n",
            "loss: 0.721634  [44864/60000]\n",
            "loss: 0.704187  [51264/60000]\n",
            "loss: 0.660053  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.609923  [   64/60000]\n",
            "loss: 0.712923  [ 6464/60000]\n",
            "loss: 0.481492  [12864/60000]\n",
            "loss: 0.723567  [19264/60000]\n",
            "loss: 0.634780  [25664/60000]\n",
            "loss: 0.626105  [32064/60000]\n",
            "loss: 0.675064  [38464/60000]\n",
            "loss: 0.713228  [44864/60000]\n",
            "loss: 0.694532  [51264/60000]\n",
            "loss: 0.647425  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.592635  [   64/60000]\n",
            "loss: 0.696493  [ 6464/60000]\n",
            "loss: 0.467942  [12864/60000]\n",
            "loss: 0.712456  [19264/60000]\n",
            "loss: 0.626888  [25664/60000]\n",
            "loss: 0.617370  [32064/60000]\n",
            "loss: 0.660047  [38464/60000]\n",
            "loss: 0.705911  [44864/60000]\n",
            "loss: 0.686186  [51264/60000]\n",
            "loss: 0.635748  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.576920  [   64/60000]\n",
            "loss: 0.681407  [ 6464/60000]\n",
            "loss: 0.455623  [12864/60000]\n",
            "loss: 0.702054  [19264/60000]\n",
            "loss: 0.619782  [25664/60000]\n",
            "loss: 0.609590  [32064/60000]\n",
            "loss: 0.646158  [38464/60000]\n",
            "loss: 0.699552  [44864/60000]\n",
            "loss: 0.678923  [51264/60000]\n",
            "loss: 0.624847  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.562688  [   64/60000]\n",
            "loss: 0.667587  [ 6464/60000]\n",
            "loss: 0.444448  [12864/60000]\n",
            "loss: 0.692278  [19264/60000]\n",
            "loss: 0.613330  [25664/60000]\n",
            "loss: 0.602606  [32064/60000]\n",
            "loss: 0.633384  [38464/60000]\n",
            "loss: 0.694114  [44864/60000]\n",
            "loss: 0.672701  [51264/60000]\n",
            "loss: 0.614670  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.549632  [   64/60000]\n",
            "loss: 0.654892  [ 6464/60000]\n",
            "loss: 0.434193  [12864/60000]\n",
            "loss: 0.683010  [19264/60000]\n",
            "loss: 0.607326  [25664/60000]\n",
            "loss: 0.596258  [32064/60000]\n",
            "loss: 0.621650  [38464/60000]\n",
            "loss: 0.689577  [44864/60000]\n",
            "loss: 0.667385  [51264/60000]\n",
            "loss: 0.605031  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.537549  [   64/60000]\n",
            "loss: 0.643234  [ 6464/60000]\n",
            "loss: 0.424764  [12864/60000]\n",
            "loss: 0.674227  [19264/60000]\n",
            "loss: 0.601704  [25664/60000]\n",
            "loss: 0.590396  [32064/60000]\n",
            "loss: 0.610832  [38464/60000]\n",
            "loss: 0.685889  [44864/60000]\n",
            "loss: 0.662870  [51264/60000]\n",
            "loss: 0.595770  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.526361  [   64/60000]\n",
            "loss: 0.632477  [ 6464/60000]\n",
            "loss: 0.416065  [12864/60000]\n",
            "loss: 0.665885  [19264/60000]\n",
            "loss: 0.596305  [25664/60000]\n",
            "loss: 0.584924  [32064/60000]\n",
            "loss: 0.600880  [38464/60000]\n",
            "loss: 0.682982  [44864/60000]\n",
            "loss: 0.658998  [51264/60000]\n",
            "loss: 0.586832  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.515912  [   64/60000]\n",
            "loss: 0.622528  [ 6464/60000]\n",
            "loss: 0.407992  [12864/60000]\n",
            "loss: 0.657920  [19264/60000]\n",
            "loss: 0.591014  [25664/60000]\n",
            "loss: 0.579707  [32064/60000]\n",
            "loss: 0.591749  [38464/60000]\n",
            "loss: 0.680763  [44864/60000]\n",
            "loss: 0.655664  [51264/60000]\n",
            "loss: 0.578240  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.506093  [   64/60000]\n",
            "loss: 0.613312  [ 6464/60000]\n",
            "loss: 0.400458  [12864/60000]\n",
            "loss: 0.650337  [19264/60000]\n",
            "loss: 0.585709  [25664/60000]\n",
            "loss: 0.574723  [32064/60000]\n",
            "loss: 0.583348  [38464/60000]\n",
            "loss: 0.679168  [44864/60000]\n",
            "loss: 0.652783  [51264/60000]\n",
            "loss: 0.569930  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.496820  [   64/60000]\n",
            "loss: 0.604749  [ 6464/60000]\n",
            "loss: 0.393439  [12864/60000]\n",
            "loss: 0.643064  [19264/60000]\n",
            "loss: 0.580457  [25664/60000]\n",
            "loss: 0.569900  [32064/60000]\n",
            "loss: 0.575550  [38464/60000]\n",
            "loss: 0.678018  [44864/60000]\n",
            "loss: 0.650195  [51264/60000]\n",
            "loss: 0.561849  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.488048  [   64/60000]\n",
            "loss: 0.596835  [ 6464/60000]\n",
            "loss: 0.386876  [12864/60000]\n",
            "loss: 0.636147  [19264/60000]\n",
            "loss: 0.575264  [25664/60000]\n",
            "loss: 0.565209  [32064/60000]\n",
            "loss: 0.568299  [38464/60000]\n",
            "loss: 0.677273  [44864/60000]\n",
            "loss: 0.647830  [51264/60000]\n",
            "loss: 0.554010  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.479751  [   64/60000]\n",
            "loss: 0.589463  [ 6464/60000]\n",
            "loss: 0.380715  [12864/60000]\n",
            "loss: 0.629561  [19264/60000]\n",
            "loss: 0.570011  [25664/60000]\n",
            "loss: 0.560627  [32064/60000]\n",
            "loss: 0.561592  [38464/60000]\n",
            "loss: 0.676920  [44864/60000]\n",
            "loss: 0.645677  [51264/60000]\n",
            "loss: 0.546420  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.471861  [   64/60000]\n",
            "loss: 0.582621  [ 6464/60000]\n",
            "loss: 0.374944  [12864/60000]\n",
            "loss: 0.623211  [19264/60000]\n",
            "loss: 0.564762  [25664/60000]\n",
            "loss: 0.556135  [32064/60000]\n",
            "loss: 0.555389  [38464/60000]\n",
            "loss: 0.676915  [44864/60000]\n",
            "loss: 0.643743  [51264/60000]\n",
            "loss: 0.539069  [57664/60000]\n",
            "completed training in ... 275.2974352836609s\n",
            "starting timer for testing using GPU...\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.550396 \n",
            "\n",
            "completed testing in ... 1.2979652881622314s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = NeuralNetwork().to(tpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using TPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, tpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using TPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, tpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "id": "Uqv4sr6eo-7y",
        "outputId": "b1bebd41-2034-4f6b-e704-38b399a84c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15993523.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 268308.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5026933.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5160678.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6cb0fa07b8ee>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;31m# Move your model to mps just like any other device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for xla devices"
          ]
        }
      ]
    }
  ]
}