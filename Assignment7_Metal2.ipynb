{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twalker47/helloAI/blob/main/Assignment7_Metal2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4B8VAohPm-2",
        "outputId": "7b3ba491-76b6-4592-f1b9-71c86d1d7c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training ConvNetwork using MPS...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.291488  [   64/60000]\n",
            "loss: 0.804220  [ 6464/60000]\n",
            "loss: 0.522948  [12864/60000]\n",
            "loss: 0.736391  [19264/60000]\n",
            "loss: 0.703648  [25664/60000]\n",
            "loss: 0.552021  [32064/60000]\n",
            "loss: 0.612976  [38464/60000]\n",
            "loss: 0.583304  [44864/60000]\n",
            "loss: 0.614265  [51264/60000]\n",
            "loss: 0.615113  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.494170  [   64/60000]\n",
            "loss: 0.492436  [ 6464/60000]\n",
            "loss: 0.376627  [12864/60000]\n",
            "loss: 0.573056  [19264/60000]\n",
            "loss: 0.534395  [25664/60000]\n",
            "loss: 0.469756  [32064/60000]\n",
            "loss: 0.417036  [38464/60000]\n",
            "loss: 0.624534  [44864/60000]\n",
            "loss: 0.630882  [51264/60000]\n",
            "loss: 0.496289  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.381323  [   64/60000]\n",
            "loss: 0.427106  [ 6464/60000]\n",
            "loss: 0.342517  [12864/60000]\n",
            "loss: 0.537277  [19264/60000]\n",
            "loss: 0.455085  [25664/60000]\n",
            "loss: 0.468331  [32064/60000]\n",
            "loss: 0.358392  [38464/60000]\n",
            "loss: 0.613957  [44864/60000]\n",
            "loss: 0.614194  [51264/60000]\n",
            "loss: 0.436582  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.321295  [   64/60000]\n",
            "loss: 0.338281  [ 6464/60000]\n",
            "loss: 0.290987  [12864/60000]\n",
            "loss: 0.470772  [19264/60000]\n",
            "loss: 0.417664  [25664/60000]\n",
            "loss: 0.472552  [32064/60000]\n",
            "loss: 0.328359  [38464/60000]\n",
            "loss: 0.622516  [44864/60000]\n",
            "loss: 0.552306  [51264/60000]\n",
            "loss: 0.388151  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.281720  [   64/60000]\n",
            "loss: 0.288796  [ 6464/60000]\n",
            "loss: 0.261956  [12864/60000]\n",
            "loss: 0.426570  [19264/60000]\n",
            "loss: 0.368997  [25664/60000]\n",
            "loss: 0.462695  [32064/60000]\n",
            "loss: 0.327343  [38464/60000]\n",
            "loss: 0.592528  [44864/60000]\n",
            "loss: 0.525058  [51264/60000]\n",
            "loss: 0.383540  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.252270  [   64/60000]\n",
            "loss: 0.259478  [ 6464/60000]\n",
            "loss: 0.234418  [12864/60000]\n",
            "loss: 0.415220  [19264/60000]\n",
            "loss: 0.322040  [25664/60000]\n",
            "loss: 0.467093  [32064/60000]\n",
            "loss: 0.323891  [38464/60000]\n",
            "loss: 0.542534  [44864/60000]\n",
            "loss: 0.474253  [51264/60000]\n",
            "loss: 0.348602  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.235097  [   64/60000]\n",
            "loss: 0.263432  [ 6464/60000]\n",
            "loss: 0.209988  [12864/60000]\n",
            "loss: 0.391999  [19264/60000]\n",
            "loss: 0.274659  [25664/60000]\n",
            "loss: 0.462243  [32064/60000]\n",
            "loss: 0.309672  [38464/60000]\n",
            "loss: 0.508024  [44864/60000]\n",
            "loss: 0.449009  [51264/60000]\n",
            "loss: 0.353200  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.217955  [   64/60000]\n",
            "loss: 0.264540  [ 6464/60000]\n",
            "loss: 0.192940  [12864/60000]\n",
            "loss: 0.381116  [19264/60000]\n",
            "loss: 0.251908  [25664/60000]\n",
            "loss: 0.445452  [32064/60000]\n",
            "loss: 0.275468  [38464/60000]\n",
            "loss: 0.467452  [44864/60000]\n",
            "loss: 0.410237  [51264/60000]\n",
            "loss: 0.354868  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.225632  [   64/60000]\n",
            "loss: 0.256132  [ 6464/60000]\n",
            "loss: 0.174841  [12864/60000]\n",
            "loss: 0.357170  [19264/60000]\n",
            "loss: 0.236885  [25664/60000]\n",
            "loss: 0.428644  [32064/60000]\n",
            "loss: 0.268951  [38464/60000]\n",
            "loss: 0.435638  [44864/60000]\n",
            "loss: 0.357645  [51264/60000]\n",
            "loss: 0.339727  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.216398  [   64/60000]\n",
            "loss: 0.239341  [ 6464/60000]\n",
            "loss: 0.177853  [12864/60000]\n",
            "loss: 0.346411  [19264/60000]\n",
            "loss: 0.228440  [25664/60000]\n",
            "loss: 0.407716  [32064/60000]\n",
            "loss: 0.245084  [38464/60000]\n",
            "loss: 0.392058  [44864/60000]\n",
            "loss: 0.338838  [51264/60000]\n",
            "loss: 0.337272  [57664/60000]\n",
            "completed training in ... 224.3595325946808s\n",
            "starting timer for testing using MPS...\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.363549 \n",
            "\n",
            "completed testing in ... 2.2023439407348633s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define convolutional model\n",
        "# Build the neural network, expand on top of nn.Module\n",
        "# adapted from https://towardsdatascience.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582\n",
        "class ConvNetwork(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\t\n",
        "\t\t# define layers\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "\t\tself.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\t\t\n",
        "\t\tself.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
        "\t\tself.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "\t\tself.out = nn.Linear(in_features=60, out_features=10)\n",
        "\n",
        "\t# define forward function\n",
        "\tdef forward(self, t):\n",
        "\t\t# conv 1\n",
        "\t\tt = t.to(gpu_device)\n",
        "\t\tt = self.conv1(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\tt = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\t\t\n",
        "\t\t# conv 2\n",
        "\t\tt = self.conv2(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\tt = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\t\t\n",
        "\t\t# fc1\n",
        "\t\tt = t.reshape(-1, 12*4*4)\n",
        "\t\tt = self.fc1(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\t\n",
        "\t\t# fc2\n",
        "\t\tt = self.fc2(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\t\n",
        "\t\t# output\n",
        "\t\tt = self.out(t)\n",
        "\t\t# don't need softmax here since we'll use cross-entropy as activation.\n",
        "\t\treturn t\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        #do not use the next two lines with CPU, just with GPU\n",
        "        #pred = pred.cuda()\n",
        "        y = y.to(cpu_device)\n",
        "        loss = F.cross_entropy(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "#gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = ConvNetwork().to(cpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(\"starting timer for training ConvNetwork using MPS...\")\n",
        "start = time.time()\n",
        "epochs = 10 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, cpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using MPS...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, cpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-pIv4LQXp4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f45d73f-56a7-499b-be78-81dc312ee962",
        "id": "VWeU2KYFXvAP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16189426.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 263342.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5081600.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 21570706.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "starting timer for training ConvNetwork using MPS...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.312956  [   64/60000]\n",
            "loss: 0.970070  [ 6464/60000]\n",
            "loss: 0.631019  [12864/60000]\n",
            "loss: 0.814435  [19264/60000]\n",
            "loss: 0.798641  [25664/60000]\n",
            "loss: 0.667965  [32064/60000]\n",
            "loss: 0.635808  [38464/60000]\n",
            "loss: 0.600234  [44864/60000]\n",
            "loss: 0.612972  [51264/60000]\n",
            "loss: 0.645317  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.551908  [   64/60000]\n",
            "loss: 0.554844  [ 6464/60000]\n",
            "loss: 0.355497  [12864/60000]\n",
            "loss: 0.662255  [19264/60000]\n",
            "loss: 0.627460  [25664/60000]\n",
            "loss: 0.513136  [32064/60000]\n",
            "loss: 0.530378  [38464/60000]\n",
            "loss: 0.552476  [44864/60000]\n",
            "loss: 0.594279  [51264/60000]\n",
            "loss: 0.524821  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.407913  [   64/60000]\n",
            "loss: 0.496127  [ 6464/60000]\n",
            "loss: 0.312369  [12864/60000]\n",
            "loss: 0.578539  [19264/60000]\n",
            "loss: 0.579660  [25664/60000]\n",
            "loss: 0.462270  [32064/60000]\n",
            "loss: 0.478862  [38464/60000]\n",
            "loss: 0.590217  [44864/60000]\n",
            "loss: 0.579743  [51264/60000]\n",
            "loss: 0.478314  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.363722  [   64/60000]\n",
            "loss: 0.406763  [ 6464/60000]\n",
            "loss: 0.271684  [12864/60000]\n",
            "loss: 0.534242  [19264/60000]\n",
            "loss: 0.493541  [25664/60000]\n",
            "loss: 0.415721  [32064/60000]\n",
            "loss: 0.465009  [38464/60000]\n",
            "loss: 0.578206  [44864/60000]\n",
            "loss: 0.545561  [51264/60000]\n",
            "loss: 0.441548  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.319556  [   64/60000]\n",
            "loss: 0.388205  [ 6464/60000]\n",
            "loss: 0.247141  [12864/60000]\n",
            "loss: 0.507865  [19264/60000]\n",
            "loss: 0.468293  [25664/60000]\n",
            "loss: 0.387582  [32064/60000]\n",
            "loss: 0.463644  [38464/60000]\n",
            "loss: 0.586893  [44864/60000]\n",
            "loss: 0.524354  [51264/60000]\n",
            "loss: 0.408278  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.296049  [   64/60000]\n",
            "loss: 0.354916  [ 6464/60000]\n",
            "loss: 0.232590  [12864/60000]\n",
            "loss: 0.480598  [19264/60000]\n",
            "loss: 0.393338  [25664/60000]\n",
            "loss: 0.371497  [32064/60000]\n",
            "loss: 0.379677  [38464/60000]\n",
            "loss: 0.578428  [44864/60000]\n",
            "loss: 0.476021  [51264/60000]\n",
            "loss: 0.398181  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.280140  [   64/60000]\n",
            "loss: 0.338366  [ 6464/60000]\n",
            "loss: 0.227438  [12864/60000]\n",
            "loss: 0.466986  [19264/60000]\n",
            "loss: 0.375466  [25664/60000]\n",
            "loss: 0.373661  [32064/60000]\n",
            "loss: 0.350000  [38464/60000]\n",
            "loss: 0.583017  [44864/60000]\n",
            "loss: 0.453214  [51264/60000]\n",
            "loss: 0.376045  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.271227  [   64/60000]\n",
            "loss: 0.338228  [ 6464/60000]\n",
            "loss: 0.227447  [12864/60000]\n",
            "loss: 0.455748  [19264/60000]\n",
            "loss: 0.328075  [25664/60000]\n",
            "loss: 0.351759  [32064/60000]\n",
            "loss: 0.335614  [38464/60000]\n",
            "loss: 0.570671  [44864/60000]\n",
            "loss: 0.449867  [51264/60000]\n",
            "loss: 0.368260  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.256220  [   64/60000]\n",
            "loss: 0.328528  [ 6464/60000]\n",
            "loss: 0.222722  [12864/60000]\n",
            "loss: 0.437581  [19264/60000]\n",
            "loss: 0.310784  [25664/60000]\n",
            "loss: 0.359091  [32064/60000]\n",
            "loss: 0.325100  [38464/60000]\n",
            "loss: 0.550394  [44864/60000]\n",
            "loss: 0.443141  [51264/60000]\n",
            "loss: 0.351666  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.247202  [   64/60000]\n",
            "loss: 0.332416  [ 6464/60000]\n",
            "loss: 0.210053  [12864/60000]\n",
            "loss: 0.414524  [19264/60000]\n",
            "loss: 0.288259  [25664/60000]\n",
            "loss: 0.348718  [32064/60000]\n",
            "loss: 0.299698  [38464/60000]\n",
            "loss: 0.531641  [44864/60000]\n",
            "loss: 0.391645  [51264/60000]\n",
            "loss: 0.323363  [57664/60000]\n",
            "completed training in ... 103.76083064079285s\n",
            "starting timer for testing using MPS...\n",
            "Test Error: \n",
            " Accuracy: 86.0%, Avg loss: 0.387851 \n",
            "\n",
            "completed testing in ... 2.3809545040130615s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define convolutional model\n",
        "# Build the neural network, expand on top of nn.Module\n",
        "# adapted from https://towardsdatascience.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582\n",
        "class ConvNetwork(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\t\n",
        "\t\t# define layers\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "\t\tself.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\t\t\n",
        "\t\tself.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
        "\t\tself.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "\t\tself.out = nn.Linear(in_features=60, out_features=10)\n",
        "\n",
        "\t# define forward function\n",
        "\tdef forward(self, t):\n",
        "\t\t# conv 1\n",
        "\t\tt = t.to(gpu_device)\n",
        "\t\tt = self.conv1(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\tt = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\t\t\n",
        "\t\t# conv 2\n",
        "\t\tt = self.conv2(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\tt = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\t\t\n",
        "\t\t# fc1\n",
        "\t\tt = t.reshape(-1, 12*4*4)\n",
        "\t\tt = self.fc1(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\t\n",
        "\t\t# fc2\n",
        "\t\tt = self.fc2(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\t\n",
        "\t\t# output\n",
        "\t\tt = self.out(t)\n",
        "\t\t# don't need softmax here since we'll use cross-entropy as activation.\n",
        "\t\treturn t\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        #do not use the next two lines with CPU, just with GPU\n",
        "        pred = pred.cuda()\n",
        "        y = y.to(gpu_device)\n",
        "        loss = F.cross_entropy(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "#gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = ConvNetwork().to(gpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(\"starting timer for training ConvNetwork using MPS...\")\n",
        "start = time.time()\n",
        "epochs = 10 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, gpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using MPS...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, gpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "81d0e558-bb85-4258-a096-748859f2f67e",
        "id": "wlLm8cdmXqVD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torch 2.0.0\n",
            "Uninstalling torch-2.0.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.9/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.9/dist-packages/nvfuser/*\n",
            "    /usr/local/lib/python3.9/dist-packages/torch-2.0.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.9/dist-packages/torchgen/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcublas.so.11\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcublasLt.so.11\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudart-d0da41ae.so.11.0\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn_adv_infer.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn_adv_train.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn_cnn_infer.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn_cnn_train.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn_ops_infer.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libcudnn_ops_train.so.8\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libnvToolsExt-847d78f2.so.1\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libnvrtc-672ee683.so.11.2\n",
            "    /usr/local/lib/python3.9/dist-packages/torch/lib/libnvrtc-builtins.so.11.8\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch-2.0.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m152.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.15.1-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading torchaudio-2.0.1-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m135.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m148.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m228.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m165.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.12rc1-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m161.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel\n",
            "  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lit\n",
            "  Downloading lit-16.0.1.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cmake\n",
            "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m163.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m144.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m237.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m164.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m200.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.1-py3-none-any.whl size=88188 sha256=f51eed3015c51187ec82bb83325c0759261e7d04cdf06535d86417e41bfb7ce5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-agf_cyen/wheels/6a/35/e6/2ba4f52d2763592eb8474f60e192f06e3d57be198526ef1048\n",
            "Successfully built lit\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: mpmath, lit, cmake, wheel, urllib3, typing-extensions, sympy, setuptools, pillow, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, networkx, MarkupSafe, idna, filelock, charset-normalizer, certifi, requests, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jinja2, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, torchaudio\n",
            "\u001b[33m  WARNING: The script lit is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts cmake, cpack and ctest are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script wheel is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script isympy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.9 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\n",
            "orbax 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n",
            "flax 0.6.8 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 certifi-2022.12.7 charset-normalizer-3.1.0 cmake-3.26.3 filelock-3.11.0 idna-3.4 jinja2-3.1.2 lit-16.0.1 mpmath-1.3.0 networkx-3.1 numpy-1.24.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pillow-9.5.0 requests-2.28.2 setuptools-67.6.1 sympy-1.12rc1 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1 triton-2.0.0 typing-extensions-4.5.0 urllib3-1.26.15 wheel-0.40.0\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bd7922afa1d2>\u001b[0m in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Move your model to mps just like any other device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for xla devices"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "!pip uninstall torch\n",
        "!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu111/torch_stable.html --user --upgrade --force-reinstall --no-cache-dir --pre\n",
        "\n",
        "\n",
        "# Define convolutional model\n",
        "# Build the neural network, expand on top of nn.Module\n",
        "# adapted from https://towardsdatascience.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582\n",
        "class ConvNetwork(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\t\n",
        "\t\t# define layers\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "\t\tself.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\t\t\n",
        "\t\tself.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
        "\t\tself.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "\t\tself.out = nn.Linear(in_features=60, out_features=10)\n",
        "\n",
        "\t# define forward function\n",
        "\tdef forward(self, t):\n",
        "\t\t# conv 1\n",
        "\t\tt = t.to(gpu_device)\n",
        "\t\tt = self.conv1(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\tt = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\t\t\n",
        "\t\t# conv 2\n",
        "\t\tt = self.conv2(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\tt = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\t\t\n",
        "\t\t# fc1\n",
        "\t\tt = t.reshape(-1, 12*4*4)\n",
        "\t\tt = self.fc1(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\t\n",
        "\t\t# fc2\n",
        "\t\tt = self.fc2(t)\n",
        "\t\tt = F.relu(t)\n",
        "\t\t\n",
        "\t\t# output\n",
        "\t\tt = self.out(t)\n",
        "\t\t# don't need softmax here since we'll use cross-entropy as activation.\n",
        "\t\treturn t\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        #do not use the next two lines with CPU, just with GPU\n",
        "        #pred = pred.cuda()\n",
        "        y = y.to(cpu_device)\n",
        "        loss = F.cross_entropy(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "#gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n",
        "model = ConvNetwork().to(tpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(\"starting timer for training ConvNetwork using MPS...\")\n",
        "start = time.time()\n",
        "epochs = 10 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, tpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using MPS...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, tpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWroeFWeQqy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}